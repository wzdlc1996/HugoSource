---
title: "Random Variable"
date: 2019-10-29T11:36:42+08:00
draft: true
tags: ["math"]
series: ["Applied-Mathematics"]
categories: ["Mathematics"]
toc: true
summary: "This is an introduction to Probability Theory. Include the definition of Probability Space and Random Variables. Conditional Expectation and How to generate random numbers. Read this would be helpful for understanding further probability theory even Measure theory in the future"
---

# Line Optimization Algorithm

## Algorithm Structure

All `line optimization algorithm` for generic question: \(\min f(x) \ ; \ x\in D\) have the following iterative form:

$$x_{k+1} = x_k + \alpha_k d_k$$

### Convergence

1.  **Global convergence** : \(\forall x_0 \in D\) , the series converges to the same point \(x^*\)
2.  **Local convergence** : \(\exists D^* \subset D\) , and \(\forall x_) \in D^*\) the series converges to the same point \(x^*\)

### Convergence rate

1.  **\(Q\)-Convergence rate**

    If \(\exists p \geq 0 , q\gt 0\) let:

    $$\lim_{k\rightarrow \infty} \frac {||x_{k+1}-x^*||} {||x_k - x^*||^p} = q = Q_p(\{x_k\})$$

    We call series \(\{x_k\}\) is \(Q_p\) convergence.

    *  If \(p=1, q\in (0,1)\) , it is \(Q\)-linear convergence. e.g. \(1+2^{-k}\)
    *  If \(p=1, q=0\) , it is super \(Q\)-linear convergence. e.g. \(1 + k^{-k}\)
    *  If \(p=1, q=1\) , it is \(Q\)-sub-linear convergence. e.g. \(1/k\)
    *  If \(p=2, q\in (0,1)\) , it is \(Q\)-quadratically convergence. e.g. \(1+2^{-2^k}\)\)

    Specially, If \(\exists q \in (0,1)\) let:

    $$\forall k : \frac {||x_{k+1}-x^*||} {||x_k - x^*||} \leq q $$

    Then the series is Q-linear.

    _Theorem_ : Let \(\lim x_k = x^*\) , denote \(h_k = x_k - x^*\) , \(s_k = x_{k+1}-x_k\). Then \(\{x_k\}\) is super-linear converge, if and only if one of the following sentences is true:
    1.  \(h_{k+1} = o(||h_k||)\Leftrightarrow \lim ||h_{k+1}||/||h_k|| = 0\)
    2.  \(s_k = - h_k + o(||h_k||)\)
    3.  \(h_k = o(||s_k||)\)
    4.  \(s_k = -h_k + o(||s_k||)\)

    _Definition_ : We define the \(Q\)-factor of iterative process \(\mathscr{P} = (x_{k+1} = f(x_k))\) at \(x^*\) . Let \(C(\mathscr{P},x^*)\) is the set of all series generated by \(\mathscr{P}\) with limit point is \(x^*\) , then:

    $$Q_p (\mathscr{P}, x^*) = \sup \{Q_p(\{x_k\}) \ : \ \{x_k\} \in C(\mathscr{P},x^*)\}$$

    is the \(Q\)-factor of iterative process \(\mathscr{P}\) at point \(x^*\)

    _Definition_ : Consider two iterative process \(\mathscr{P}_1, \mathscr{P}_2\) , if \(\exists p\in[1,\infty)\) , let :

    $$Q_p(\mathscr{P}_1,x^*) \lt Q_p(\mathscr{P}_2,x^*)$$

    Then we call \(\mathscr{P}_1\) is \(Q\)-faster than \(\mathscr{P}_2\) .

    1.  There is no  \(p,p'\in[1,\infty)\) letting \(Q_p(\mathscr{P}_1,x^*) \lt Q_p(\mathscr{P}_2,x^*)\) and \(Q_{p'}(\mathscr{P}_1,x^*) \gt Q_{p'}(\mathscr{P}_2,x^*)\)
    2.  If \(\mathscr{P}_1\) is \(Q\)-faster than \(\mathscr{P}_2\) , \(\mathscr{P}_2\) is \(Q\)-faster than \(\mathscr{P}_3\) , then \(\mathscr{P}_1\) is \(Q\)-faster than \(\mathscr{P}_3\)
    3.  The property: \(Q\)-faster is dependent of the definition of norm.

2.  **\(R\)-convergence rate**

    If \(\exists p \geq 0, R\gt 0\) let:

    $$R_p = \begin{cases}\lim_{k\rightarrow \infty} \sup \{\|x_n-x^*\|^{1/R}\}_{n\geq k} & p=1 \\ \lim_{k\rightarrow \infty} \sup \{\|x_k-x^*\|^{1/p^R}\}_{n\geq k} & p\gt 1 \\\end{cases}$$

    *  If \(p=1, R\in (0,1)\) , it is \(R\)-linear convergence.
    *  If \(p=1, R=0\) , it is super \(R\)-linear convergence.
    *  If \(p=1, R=1\) , it is \(R\)-sub-linear convergence.
    *  If \(p=2, R\in (0,1)\) , it is \(R\)-quadratically convergence.

    \(R\)-convergence do not need the series \(\| x_k - x^*\|\) is monotonous. e.g. \(x_k = \begin{cases} 1+ 2^{-k} & k \text{ is odd} \\ 1 & k \text{ is even}\end{cases}\)

    \(R\)-convergence can also be defined as an extension for \(Q\)-convergence: If there exists \(v_k\) such that:

    $$\|x_k - x^*\|\leq v_k$$

    Then if \(v_k\) converges to zero with order \(q\) in the sense of Q-convergence, we call \(x_k\) is \(R\)-convergence.

## Exact Line Search

We assume the descent orientation \(d_k\) has been got by other algorithm, we need to find the way to determine the descent rate \(\alpha_k\) .

In `exact line search` , \(\alpha_k\) is:

$$\alpha_k = \arg \min_\alpha f(x_k + \alpha d_k)\equiv \arg \min_\alpha \phi(\alpha)$$

Usually, we assume there exists a interval \([a,b]\) on which \(\phi(\alpha)\) has a unique local minimum. i.e., \(\exists \alpha^* \in [a,b]\) , on \([a,\alpha^*]\) function \(\phi(\alpha)\) is monotonous decrease, and on \([\alpha^*,b]\) function \(\phi(\alpha)\) is monotonous increase.

Then we have the property, if \(\forall \alpha_1, \alpha_2 \in [a,b]\) and \(\alpha_1\lt \alpha_2\) :
1.  \(\phi(\alpha_1) \lt \phi(\alpha)\) , then \(\alpha^* \in [a,\alpha_2]\)
2.  \(\phi(\alpha_1) \gt \phi(\alpha)\) , then \(\alpha^* \in [\alpha_1,b]\)
3.  \(\phi(\alpha_1) = \phi(\alpha)\) , then \(\alpha^* \in [\alpha_1,\alpha_2]\)

Now we have the algorithm for finding the search interval:

```python
alpha, h, t, i # Initialization
alpha = alpha + h
if phi(alpha) < phi(alpha) :
    h = t * h
    alpha -= h / t
    i = i + 1
    goto 2
else:
    if i == 1 :
        h = -h
    alpha -= h/t
end with: a = min(alpha, alpha+h) , b = max(alpha, alpha + h)
```

golden ratio, 进退法找到搜索区间,

## Convergence of ELS

### Objective function descent

_[Theorem]_ : Let \(\alpha_k = \arg \min_{\alpha} \phi(\alpha)\) , is the result of ELS. Assume \(\|G(x_k + \alpha_k d_k)\| \lt M\) , i.e., \(\|x_{k+1}\|\) is bounded. Then:

$$f(x_k) - f(x_{k+1}) \geq \frac 1 {2M} \|g_k\|^2 \cos^2 \theta_k \ ; \ \theta_k = <d_k, -g_k>$$

Where \(g_k = (\nabla f)(x_k) , G_k = (\partial_i \partial_j f)(x_k)\)

_[Proof]_ : With the fact that \(\phi(\alpha_k) \leq \phi(\alpha) \forall \alpha\) , so:

$$\begin{aligned}
f(x_k) - f(x_{k+1}) &\geq f(x_k) - f(x_k+\alpha d_k) \\
&\geq -\alpha g_k^T d_k - \frac 1 2 M \|d_k\|^2 \alpha^2 \\
&= -\frac 1 2 M \|d_k\|^2 (\alpha + \frac { g_k^T d_k} {M \|d_k\|^2})^2 + \frac { (g_k^T d_k)^2} {2M \|d_k\|^2} \\
&\geq \frac 1 {2M} \|g_k\|^2 \big(\frac {g_k \cdot d_k} {\|g_k\|\|d_k\|}\big)^2
\end{aligned}$$

q.e.d.

_[Theorem]_ : Assume \(g(x)\) is uniformly continuous on set \(L=\{x : f(x)\leq f(x_0)\}\). And \(\theta_k \leq \frac \pi 2 -\mu\) where \(\mu \gt 0\), then one of the following statement is true:
1.  \(g_k = 0\) for some \(k\)
2.  \(f_k \rightarrow -\infty\)
3.  \(g_k \rightarrow 0\)

_[Proof]_ : Assume 1 and 2 are false. According to the theorem above, series \(\{f_k\}\) should be monotonous descent and has a lower bound: \(f_{k+1}\lt f_k \ ; \ \exists M \Rightarrow f_k \geq M\).

If \(g_k\) does not converge to zero, then \(\exists \epsilon\gt 0\Rightarrow \|g_k\| \geq \epsilon\).

Now:

$$\begin{aligned}
f(x_k + \alpha_k d_k) &\leq f(x_k + \alpha d_k) \\
&= f(x_k) + \alpha g(\xi_k) d_k \ \text{(Lagrange)} \\
&= f(x_k) +\alpha g_k^T d_k + \alpha (g(\xi_k) - g_k)^T d_k \\
&\leq f(x_k) + \alpha \|d_k\|\Big( \frac {g_k^T d_k} {\|d_k\|} + \|g(\xi_k) - g_k\|\Big) \\
&\leq f(x_k) + \alpha \|d_k\| ( - \epsilon \sin \mu + \|g(\xi_k) -g_k\| )
\end{aligned}$$

With the uniformly continuous (\(\forall \epsilon \gt 0, \exists \delta \gt 0 \text{ s.t. } |x_1-x_2|\leq \delta \Rightarrow |f(x_1)-f(x_2)|\leq \epsilon\)), there exists \(\bar \alpha\) s.t. \(l \leq \alpha \|d_k\| \leq \bar \alpha\):
$$\|g(\xi_k) - g_k\| \leq \frac 1 2 \epsilon \sin \mu$$

Then:

$$f(x_k + \alpha_k d_k) \leq f(x_k) - \frac 1 2 l \epsilon \sin \mu$$

Which is oppsite to \(f_k - f_{k+1} \rightarrow 0\).

## 单调不精确线搜索

设 \(\phi(\alpha)\) 在 \(\alpha=0\) 是下降的, 如果是上升的就反转搜索方向!

Goldstein准则:

$$f_k + (1-\rho) \alpha g_k^T d_k \leq f(x_k + \alpha d_k)\leq f_k +\rho \alpha g_k^T d_k$$

\(\rho\) 调节 直线斜率, 避开终端的一些点, 要求迭代点的函数值在两个直线之间

(\(\rho \in (0,1/2)\))

Wolfe 准则:
$$f(x_k + \alpha d_k)\leq f_k +\rho \alpha g_k^T d_k \ ; \ d_k^T g(x_k+\alpha d_k) \geq \sigma g_k^T d_k$$

\(\sigma \in (\rho ,1)\)

强 Wolfe 准则:

$$f(x_k + \alpha d_k)\leq f_k +\rho \alpha g_k^T d_k \ ; \ |d_k^T g(x_k+\alpha d_k)| \leq  -\sigma g_k^T d_k \ ; \ d_k^T g(x_k+\alpha d_k) \geq \sigma g_k^T d_k$$

Armyo 准则

给定 \(\alpha, \rho\in (0,1/2) , \beta\in(0,1)\) , let \(\alpha_k = \beta^{m_k} \alpha\), \(m_k\) 使

$$f(x_k + \beta^m \alpha d_k)\leq f_k + \rho \beta^m \alpha g_k^T d_k$$

的自然数 \(m\in \{0,1,\cdots\}\)
